{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/conda/lib/python3.10/site-packages (0.1.72)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.10/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/conda/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p098t4sKaIee",
    "outputId": "8cca14dd-f2b2-425a-c7aa-ed5a195b6de3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from numpy import array, average\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZahsHkb0aIeb"
   },
   "source": [
    "# NLP Assignment 1 (40% of grade): Text classification for Fake News Detection\n",
    "\n",
    "This coursework will involve you implementing functions for a text classifier, which you will train to detect **fake news** in a corpus of approx. 12,000 statements. \n",
    "\n",
    "In this template you are given the basis for that implementation, though some of the functions are missing, which you have to fill in.\n",
    "\n",
    "Follow the instructions file **NLP_Assignment_1_Instructions.pdf** for details of each question - the outline of what needs to be achieved for each question is as below.\n",
    "\n",
    "You must submit all **ipython notebooks and extra resources you need to run the code if you've added them** in the code submission, and a **2 page report (pdf)** in the report submission on QMPlus where you report your methods and findings according to the instructions file for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8rae04SaIeg"
   },
   "source": [
    "# Question 1: Input and Basic preprocessing (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_10Fv5HpaIeg"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import data\n",
    "df_fn = pd.read_csv('fake_news.tsv',sep='\\t') # Load TSV file\n",
    "tuple_list = []\n",
    "def convert_label(label):\n",
    "    \"\"\"Converts the multiple classes into two,\n",
    "    making it a binary distinction between fake news and real.\"\"\"\n",
    "    #return label\n",
    "    # Converting the multiclass labels to binary label\n",
    "    labels_map = {\n",
    "        'true': 'REAL',\n",
    "        'mostly-true': 'REAL',\n",
    "        'half-true': 'REAL',\n",
    "        'false': 'FAKE',\n",
    "        'barely-true': 'FAKE',\n",
    "        'pants-fire': 'FAKE'\n",
    "    }\n",
    "    return labels_map[label]\n",
    "\n",
    "data_line_list = df_fn.iloc[:,1] # Select list of all Datalines in line/label columns\n",
    "\n",
    "\n",
    "def parse_data_line(data_line_list):\n",
    "  # Should return a tuple of the label as just FAKE or REAL and the statement\n",
    "  # e.g. (label, statement)\n",
    "\n",
    "    data_label = data_line_list[0] # Select label column from the row\n",
    "    data_line = data_line_list[1] + data_line_list[3] + data_line_list[4] + data_line_list[5] # Select feature column\n",
    "    label = convert_label(data_label) # Convert label\n",
    "\n",
    "    return (label, data_line) # Return tuple of label and feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YAmtfPEaO0zz",
    "outputId": "3ccc44bc-47e4-4311-d404-5f1a3e2d1ea9"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import contractions\n",
    "\n",
    "\n",
    "num_tokens_text_dict = {}\n",
    "text = \"Is this, http://www.google.com : correcting feet won't?\"\n",
    "\n",
    "# Input: a string of one statement\n",
    "def pre_process(text):\n",
    "  # Add sylistic feature: count of unique words per statement. Achieved by just spliting the unprocessed text\n",
    "  num_tokens_text_dict['number'] = len(text.split())    # Pass the length of tokens to dictionary \n",
    "  # ---------------------------------------------------- \n",
    "  # Tried and rejected methods\n",
    "    \n",
    "  # Remove Lowercase & URL Removal\n",
    "  #tokens = re.sub(r'http\\S+', '', text.lower())\n",
    "\n",
    "  # Remove punctuation\n",
    "  #tokens = re.sub(r'[^\\w\\s]', '', tokens)\n",
    "\n",
    "  #------------------------------------------------------\n",
    "\n",
    "  # Split contraction\n",
    "  tokens = ' '.join([contractions.fix(word) for word in text.split()])\n",
    "  \n",
    "  # Tokenize\n",
    "  tokens = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", tokens) # separates punctuation at ends of strings\n",
    "  tokens = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", tokens) # separates punctuation at beginning of strings\n",
    "  tokens = re.split(r\"\\s+\",tokens)\n",
    "\n",
    "  \n",
    "\n",
    "  # Convert to lower case\n",
    "  tokens = [t.lower() for t in tokens]\n",
    "\n",
    "  # Stop word removal \n",
    "  stop_words = set(stopwords.words('english')) # List of stopwords in 'english' to compare with \n",
    "  filtered_sentence = []\n",
    "\n",
    "  for w in tokens:                                # For words in tokens \n",
    "    if w not in stop_words:                       # If w is not in stop_words list\n",
    "        filtered_sentence.append(w)               # Append to new list variable\n",
    "  tokens = filtered_sentence                      # Fresh list of tokens exclude stop words\n",
    "\n",
    "\n",
    "  # Stemming\n",
    "  # Create object of PorterStemmer library which removes suffixes or prefixes used with a word\n",
    "  #port = PorterStemmer()                        \n",
    "  #tokens = [port.stem(word) for word in tokens] # Pass each word through the stem method for stemming\n",
    "\n",
    "  # Lemmitization\n",
    "  # Create object of PorterStemmer library which converts the word to its base form\n",
    "  lem = WordNetLemmatizer()\n",
    "  tokens = [lem.lemmatize(words) for words in tokens] # Pass each word through the lemmatize method\n",
    "\n",
    "\n",
    "  return tokens\n",
    "\n",
    "# Test\n",
    "#print(\"This is a preprocessed sentence: \", pre_process(text))\n",
    "#print(num_tokens_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "   \n",
    "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
    "    with open(path) as f:\n",
    "\n",
    "        reader = csv.reader(f, delimiter='\\t') # Read file\n",
    "        for line in reader:                    # For each line in the file\n",
    "          #print(line)\n",
    "          if line[0] == \"Id\":                  # Skip header\n",
    "              continue\n",
    "          (label, text) = parse_data_line(line)# Return (label, text) from parse function\n",
    "          raw_data.append((text, label))       # Append it to variable as raw data\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "test1 = []\n",
    "train1 = []\n",
    "\n",
    "def split_and_preprocess_data(percentage):\n",
    "    \n",
    "    \"\"\"Split the data between train_data and test_data according to the percentage\n",
    "    and performs the preprocessing.\"\"\"\n",
    "    #print(raw_data[0:2])\n",
    "    num_samples = len(raw_data) \n",
    "    #print(num_samples)\n",
    "    num_training_samples = int((percentage * num_samples)) \n",
    "    #print(num_training_samples)\n",
    "    \n",
    "    # The text is first pre-proccessed and then converted to feature vector \n",
    "    for (text, label) in raw_data[:num_training_samples]: # for tuples in raw data from 0 to num train sample calculated\n",
    "      train1.append((text, label))                        # For printing original text in .txt analysis file\n",
    "      train_data.append((to_feature_vector(pre_process(text)),label)) # train data generated for training classifier\n",
    "    for (text, label) in raw_data[num_training_samples:]: # test data\n",
    "        test_data.append((to_feature_vector(pre_process(text)),label)) # test data generated for making predictions\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVgkTECJaIeh"
   },
   "source": [
    "# Question 2: Basic Feature Extraction (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aY_Pdxnmsm_A"
   },
   "outputs": [],
   "source": [
    "# Dictionary that has tokens for keys, and values a weight for each of those tokens in the preprocessed statements. \n",
    "from collections import Counter\n",
    "global_feature_dict = {}\n",
    "\n",
    "# Creates list of tokens using pre_processed func\n",
    "# Pre-processed tokens passed through to_feature_vector function\n",
    "def to_feature_vector(tokens):\n",
    "\n",
    "  # Extract the value ({number: lenght}) of the dictionary to a list\n",
    "  num_tokens_text = list(num_tokens_text_dict.values()) \n",
    "\n",
    "  #Global feature vector\n",
    "  #------------------------------------------------------------------\n",
    "  i=0\n",
    "  for word in tokens:                           # For all words in a sentence\n",
    "    if word not in global_feature_dict.keys():  # If word already in global dict\n",
    "      if bool(global_feature_dict) == False:    # If dict is empty\n",
    "        global_feature_dict[word] = 0           # Assign 0 to word\n",
    "      else:                                     # Else assign value of last element +1 \n",
    "        global_feature_dict[word] = list(global_feature_dict.values())[-1]+1  \n",
    "      i+=1                                      # Next word\n",
    "    else:\n",
    "      i+=1                                      # If word already in global dict then next\n",
    "\n",
    "  #-----------------------------------------------------------------\n",
    " \n",
    "  ngrams = Counter() # a counter for how many times a given ngram sequence w_i-1,w_i occurs\n",
    "  \n",
    "  order = 3 # Order (i.e. n) of the language model (Tried with bigram and trigram)\n",
    "  tokens = ['<s>'] * (order-1) + tokens + ['</s>'] # Add start and stop tags for ngram\n",
    "  \n",
    "\n",
    "  for i in range(order - 1, len(tokens)):  # For loop to get the ngram sequence \n",
    "      context = tokens[i-order+1:i]        # Sequence of context/(n-1)previous tokens\n",
    "      target = tokens[i]                   # The token nth token\n",
    "      ngram_sequence = context + [target]  # Final n-gram sequence is the combination of (n-1)context + nth target \n",
    "      \n",
    "      ngrams[' '.join(ngram_sequence)] += 1  # Count how many times the sequence occurs\n",
    "      ngram_dict = ngrams\n",
    "\n",
    "  \n",
    "  \n",
    "  ngram_dict['No.of tokens'] = num_tokens_text[0]  # Add sylistic feature: Number of unique words per feature\n",
    "  \n",
    "  return ngram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRf1dhuf0X-t",
    "outputId": "d1a771a2-6257-4794-bda2-45ac3dadfb90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight Counter({'No.of tokens': 18, 'lord mercy lord': 2, 'mercy lord mercy': 2, '<s> <s> lord': 1, '<s> lord mercy': 1, 'lord mercy old': 1, 'mercy old two': 1, 'old two ,': 1, 'two , finish': 1, ', finish </s>': 1})\n",
      "weight Counter({'No.of tokens': 5, '<s> <s> lord': 1, '<s> lord mercy': 1, 'lord mercy flower': 1, 'mercy flower </s>': 1})\n",
      "Global: {'lord': 0, 'mercy': 1, 'old': 2, 'two': 3, ',': 4, 'finish': 5, 'flower': 6} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test to_feature vector function and global dictionary\n",
    "text = [\"lord mercy lord mercy lord mercy That old why is two it , is it as as finish\",\"lord mercy what flower why\"]\n",
    "\n",
    "for text in text:\n",
    "  print(\"weight\",to_feature_vector(pre_process(text)))\n",
    "\n",
    "print(\"Global:\", global_feature_dict,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Y7B1RgQIaIei"
   },
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def train_classifier(data):\n",
    "\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(class_weight={0:0.8,1:0.2},max_iter=10000))]) \n",
    "    # .train() method which takes a dict of featureset and labels as input\n",
    "    return SklearnClassifier(pipeline).train(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rICzZLHUk9OJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCA-HMdraIej"
   },
   "source": [
    "# Question 3: Cross-validation (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZGH9BYGlaIej"
   },
   "outputs": [],
   "source": [
    "#from traitlets.traitlets import validate\n",
    "#solution\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def cross_validate(dataset, folds):\n",
    "   \n",
    "    results = []\n",
    "    #results_avg = []\n",
    "    #accuracy = 0.0\n",
    "    fold_size = int(len(dataset)/folds) + 1\n",
    "    \n",
    "    \n",
    "    for i in range(0,len(dataset),int(fold_size)):\n",
    "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
    "        print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
    "        # FILL IN THE METHOD HERE\n",
    "        \n",
    "        # Validation fold of len(db)/10 + 1\n",
    "        validation_fold = dataset[i:i+fold_size]  \n",
    "        \n",
    "        # train-fold which is all data expect val-fold\n",
    "        train_fold= dataset[0:i] + dataset[(i+fold_size):len(dataset)] \n",
    "        # Train classifier for each fold\n",
    "        classifier = train_classifier(train_fold) \n",
    "        \n",
    "        y_pred =[]\n",
    "        y_true = []\n",
    "        for j in range(0, 812): # Range of val-fold\n",
    "\n",
    "          y_pred.append((predict_labels(validation_fold[j][0], classifier))) #Predict labels for val-fold\n",
    "          y_true.append(validation_fold[j][1]) #True labels of that val-fold\n",
    "          \n",
    "        #print(\"Total predicted labels:\",len(y_pred))\n",
    "        #print(\"Total true labels:\", len(y_true))\n",
    "        target_names = ['FAKE', 'REAL']  # For heatmap\n",
    "\n",
    "\n",
    "        # Classification report\n",
    "        a = precision_recall_fscore_support(y_true, y_pred, average='weighted') # Calculate P, r ,fscore\n",
    "        accuracy = accuracy_score(y_true, y_pred) # Calculate accuracy\n",
    "        p, r, f = a[:3]\n",
    "        results.append((p, r, f, accuracy)) # Performance result for each fold\n",
    "        \n",
    "\n",
    "        result_array = array(results)\n",
    "        # Calculate average of 'results'. (Axis=0: along the column)\n",
    "        cv_results = list(average(result_array, axis=0).round(2)) \n",
    "\n",
    "        # Heatmap for error analysis\n",
    "        #confusion_matrix_heatmap(y_true, y_pred, target_names)\n",
    "\n",
    "        \n",
    "    return results, cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "63APuhMgaIej"
   },
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predict_labels(samples, classifier):\n",
    "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
    "    return classifier.classify_many(samples)\n",
    "\n",
    "\n",
    "def predict_label_from_raw(sample, classifier):\n",
    "    \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
    "    return classifier.classify(to_feature_vector(pre_process(sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGx-dMx3aIek",
    "outputId": "4f7a5c24-5e12-4e63-c4ad-40ef729db34d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "\n",
      "Preparing the dataset...\n",
      "\n",
      "Now 10240 rawData, 0 trainData, 0 testData\n",
      "\n",
      "Preparing training and test data...\n",
      "\n",
      "After split, 10240 rawData, 8192 trainData, 2048 testData\n",
      "Training Samples: \n",
      "8192\n",
      "Features: \n",
      "16007\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "raw_data = []          # the filtered data from the dataset file\n",
    "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
    "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
    "\n",
    "df = pd.read_csv('fake_news.tsv',sep='\\t') # Load TSV file \n",
    "\n",
    "# Clean file for NaN values\n",
    "df['label'].fillna('0', inplace=True) \n",
    "df['statement'].fillna('0', inplace=True)   \n",
    "df['subject'].fillna('0', inplace=True)   \n",
    "df['speaker'].fillna('0', inplace=True)   \n",
    "df['speaker_job_title'].fillna('0', inplace=True)   \n",
    "df['state_info'].fillna('0', inplace=True)   \n",
    "df['party_affiliation'].fillna(0, inplace=True)   \n",
    "df['total_barely_true_counts'].fillna(0, inplace=True)  \n",
    "df['total_false_counts total_half_true_counts'].fillna(0, inplace=True)  \n",
    "df['total_mostly_true_counts'].fillna(0, inplace=True)  \n",
    "df['total_pants_on_fire_counts'].fillna(0, inplace=True)  \n",
    "df['context'].fillna('0', inplace=True) \n",
    "\n",
    "\n",
    "# New file created with no NaN values\n",
    "df.to_csv(\"fake_news1.tsv\", sep=\"\\t\",index=False)  \n",
    "\n",
    "# references to the data files\n",
    "data_file_path = 'fake_news1.tsv'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"\\nPreparing the dataset...\",sep='\\n')\n",
    "print()\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# cross validation is done on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"\\nPreparing training and test data...\",sep='\\n')\n",
    "print()\n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGMdninFaIet"
   },
   "source": [
    "# 4. Error Analysis (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FBHBrXjkaIet"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# a function to make the confusion matrix readable and pretty\n",
    "def confusion_matrix_heatmap(y_test, preds, labels):\n",
    "    \"\"\"Function to plot a confusion matrix\"\"\"\n",
    "    # pass labels to the confusion matrix function to ensure right order\n",
    "    cm = metrics.confusion_matrix(y_test, preds, labels=labels)\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels( labels, rotation=45)\n",
    "    ax.set_yticklabels( labels)\n",
    "\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            #print(j, i, cm[i, j])\n",
    "            text = ax.text(j, i, cm[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "    # fix for mpl bug that cuts off top/bottom of seaborn viz:\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    #plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    plt.show() # ta-da!\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyNZLuvOVjNp",
    "outputId": "a57e1437-23cc-4557-ac0e-d2a10d7258a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold start on items 0 - 820\n",
      "Training Classifier...\n"
     ]
    }
   ],
   "source": [
    "a, b = cross_validate(train_data, 10)  # will work and output overall performance of p, r, f-score when cv implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKLI7LGZVgHt",
    "outputId": "ddd1be86-6d32-4ac5-f6b7-6b511ec09fb8"
   },
   "outputs": [],
   "source": [
    "# Print Avg performance\n",
    "print(\"Avg Precision: {0}\\nAvg Recall: {1}\\nAvg Fscore: {2}\\nAvg Accuracy: {3}\\n\".format(b[0],b[1],b[2],b[3]))\n",
    "\n",
    "# Print performance for each fold\n",
    "#for i in range(0, len(a)):\n",
    "    #print(\"Fold\", i)\n",
    "    #print(\"Precision: {0}, Recall: {1}, Fscore: {2}, Accuracy: {3}\\n\".format(a[i][0].round(2),a[i][1].round(2),a[i][2].round(2),a[i][3].round(2)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "fT-o5VIM6DqQ",
    "outputId": "7ef77a4e-7bd2-4f25-9603-5cd7cdaa8a71"
   },
   "source": [
    "# Error Analysis on First fold of Train-Validation data\n",
    "\n",
    "print(\"Error Analysis on First fold of Train-Validation data-\\n\")\n",
    "\n",
    "labels = ['FAKE', 'REAL']\n",
    "y_true = []\n",
    "y_pred = []\n",
    "file_FP_fake = \"FP.txt\"   # Create a file for all False Positives for FAKE\n",
    "file_FN_fake = \"FN.txt\"   # Create a file for all False Negatives for FAKE\n",
    "\n",
    "val1 = train1[0:812]      # For raw text from first fold of validation data\n",
    "val_data = train_data[0:812] # First fold of validation data\n",
    "\n",
    "# Train classifier using first fold of train data\n",
    "classifier = train_classifier(train_data[0:0]+train_data[820:len(train_data)]) \n",
    "\n",
    "for j in range(0, len(val_data)): # iterate over val data\n",
    "    # Predictions on Val data by passing val_data tokens & classifier\n",
    "    y_pred.append(predict_labels(val_data[j][0], classifier)) \n",
    "    y_true.append(val_data[j][1]) # Actual labels of validation data\n",
    "\n",
    "y_pred = [item for sublist in y_pred for item in sublist] # Flatten y_pred list cause it's [[]] \n",
    "\n",
    "count_fp = 0\n",
    "count_fn = 0\n",
    "#ith = []\n",
    "\n",
    "# Print output of FP and FN for FAKE label in .txt file\n",
    "with open(file_FP_fake, 'w') as fp:\n",
    "    for i, pred in enumerate(y_pred): #iterate over test_pred\n",
    "        if pred == 'FAKE' and y_true[i] == 'REAL' :# If pred of ith sample is not equal to actual label \n",
    "            a =str(str(i)+'\\n'+str(val_data[i][0])+'\\nPrediction: '+pred+' -> Truth: '+y_true[i]+'\\n\\n'+'OG text--> '+val1[i][0]+'\\n\\n')\n",
    "            fp.write(a) # print tokens, pred and actual label of that sample to a file\n",
    "            count_fp +=1\n",
    "            \n",
    "with open(file_FN_fake, 'w') as fn:\n",
    "    for i, pred in enumerate(y_pred): # Iterate over test_pred\n",
    "        if pred == 'REAL' and y_true[i] == 'FAKE' :# If pred of ith sample is not equal to actual label \n",
    "            b =str(str(i)+'\\n'+str(val_data[i][0])+'\\nPrediction: '+pred+' -> Truth: '+y_true[i]+'\\n\\n'+'OG text--> '+val1[i][0]+'\\n\\n')\n",
    "            fn.write(b) # print tokens, pred and actual label of that sample to a file\n",
    "            count_fn +=1\n",
    "    \n",
    "\n",
    "print(\"Total FP for Fake:\", count_fp) # Count of FPs for FAKE for verifying with the heatmap\n",
    "print(\"Total FN for Fake:\", count_fn) # Count of FNs for FAKE for verifying with the heatmap\n",
    "\n",
    "        \n",
    "confusion_matrix_heatmap(y_true, y_pred, labels) # Precision and Recall map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dljf7EE1aIeu"
   },
   "source": [
    "# Questions 5 (20%) and 6 (20%) (recommend starting a new notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3tcSz2VaIeu"
   },
   "outputs": [],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the traning data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(test_data[0])   # have a look at the first test data instance\n",
    "    classifier = train_classifier(train_data)  # train the classifier\n",
    "    test_true = [t[1] for t in test_data]   # get the ground-truth labels from the data\n",
    "    test_pred = predict_labels([x[0] for x in test_data], classifier)  # classify the test data to get predicted labels\n",
    "    final_scores = precision_recall_fscore_support(test_true, test_pred, average='weighted') # evaluate\n",
    "    accuracy = accuracy_score(test_true, test_pred) # Calculate accuracy\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % final_scores[:3])\n",
    "    print(\"Accuracy\",accuracy)\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
